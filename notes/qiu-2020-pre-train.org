#+TITLE: Notes on: Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., & Huang, X. (2020): Pre-trained models for natural language processing: a survey

1. 表示学习的要点: 通过大量数据，学习得到通用的，任务无关的知识表示

2. context信息
   1) 上下文无关: 动态固定的，直接查表得到特征表示
   2) 上下文有关
      * CNN: 通过卷积窗关联上下文
      * RNN: 通过上下时候的连接获取上下文
      * Transformer: 上面的方式是通过人工设计上下文的获取方式，而
        Transformer则是选择全连接，然后通过attention的方式让网络自己学
        习上下文的关联方式。 

3. 发展历史
   1) word embedding
      - 直接使用词向量
      - 模型简单
      - 只有浅层特征

   2) contextual encoder
      - 使用整个模型
      - 模型比较复杂，特征比较丰富

4. 训练任务
   1) LM: 语言模型，给定历史，预测下一个单词
   2) MLM: masked LM, 随机将某些单词掩码，然后预测这些单词 (判别模型),
      训练语料和解码语料不一致
   3) Seq2Seq MLM: 随机将某些单词掩码，翻译生成被mask的单词 (生成模型)
   4) PLM: Permuted LM, 随机打乱输入中某些单词的顺序, 模型用来预测之前
      的单词， 可以解决训练语料和解码语料不一致的问题
   5) DAE: Denoising Autoencoder, 自除噪解码器，输入加噪，然后模型预测
      生成去噪之后的句子
   6) Contrasitive Learning(CTL): 区分/对比学习
      - 相比于预测目标标签，只需要将目标标签和其他标签尽可能区分开即可,
        有点类似于生成式模型和区分性模型
      - DIM: Deep Infomax, 互信息，需要深入
      - RTD: replaced token detection, 预测判断一个单词是否被替换
      - NSP: next sentence prediction, 判断两个句子是否是上下文关系
      - SOP: sentence order prediction

5. 优化
   1) model pruning: 模型裁剪, 不使用完整的网络，使用某几层
   2) quantization: 量化
   3) parameter sharing: 参数共享，多层使用同一组参数，减少模型大小，不
      减少计算量
   4) knowledge distillation
      - soft label
      - 使用中间层的输出作为监督信号
      - 使用ensembling 模型作为老师
      - 不同模型架构的蒸馏
   5) model replacing: 忒修斯之船，逐步替换模型中的模块
